# kaggle-berlin
Material of the Kaggle Berlin meetup group!

# Collection of Sources

Here a small, but growing, collection of sources that we have been discussing on our hack sessions.

Star ratings are from :star: to :star::star::star::star::star: and subject of discussions in the Kaggle group.

## Practical Tips

**[0]** Aarshay Jain: Complete Guide to Parameter Tuning in XGBoost (with codes in Python) [[link]](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/) **(XGBoost won many Kaggle competitions and is from the gradient boosted tree-based model family.)**

**[1]** HJ van Veen: Feature Engineering [[slideshare]](https://www.slideshare.net/HJvanVeen/feature-engineering-72376750) **(Read this to understand basics of preprocessing and feature engineering!)** :star::star::star::star:

## Books

**[0]** Bengio, Yoshua, Ian J. Goodfellow, and Aaron Courville. "**Deep learning**." An MIT Press book. (2015). [[pdf]](https://github.com/HFTrader/DeepLearningBook/raw/master/DeepLearningBook.pdf) **(Good theory book to get started, modern! Then go to papers.)** :star::star::star::star:

**[1]** Murphy, Kevin. "**Machine Learning**" An MIT Press book. (2012) [[link]](https://mitpress.mit.edu/books/machine-learning-0) **(Not a good starter book, comprehensive and mathematics heavy. I use this a reference manual)** :star::star::star:
